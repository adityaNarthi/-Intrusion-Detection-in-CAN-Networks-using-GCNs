{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "torch.randn(1).cpu().numpy()  # warm-up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EASSCiYPBt4p",
        "outputId": "2b34a39b-b4d8-45cc-fbda-802dc6fdfeb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.2762235], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IhyXQT4Tw0_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7cc1f6da-4d92-4166-d3a4-6e02c20818e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m529.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.15.2+cu118\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.0.2\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.0.2%2Bcu118-cp311-cp311-linux_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.6)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2+cu118) (11.2.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1+cu118)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2+cu118) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89990 sha256=d44fb14378f4d9a4e746e24fb3cfd18b755e5c1b39637458f02b411910c7aac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.6.0+cu124\n",
            "    Uninstalling torchaudio-2.6.0+cu124:\n",
            "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Successfully installed lit-15.0.7 torch-2.0.1+cu118 torchaudio-2.0.2+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              },
              "id": "18cd7bb4229e45f296a70d628b4f888b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.4.0%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp311-cp311-linux_x86_64.whl (886 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m886.5/886.5 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt20cu118 torch_cluster-1.6.3+pt20cu118 torch_scatter-2.1.2+pt20cu118 torch_sparse-0.6.18+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting torch-geometric-temporal\n",
            "  Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (4.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.0.1+cu118)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (3.0.12)\n",
            "Requirement already satisfied: torch_sparse in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (0.6.18+pt20cu118)\n",
            "Requirement already satisfied: torch_scatter in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.1.2+pt20cu118)\n",
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch-geometric-temporal) (3.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch->torch-geometric-temporal) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch->torch-geometric-temporal) (15.0.7)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (2025.3.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric->torch-geometric-temporal) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse->torch-geometric-temporal) (1.15.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric->torch-geometric-temporal) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torch-geometric-temporal) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric->torch-geometric-temporal) (2025.4.26)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch->torch-geometric-temporal) (1.3.0)\n",
            "Downloading torch_geometric_temporal-0.56.0-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric-temporal\n",
            "Successfully installed torch-geometric-temporal-0.56.0\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch Geometric and TGAT dependencies\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-geometric-temporal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import TransformerConv, global_mean_pool, BatchNorm, GATConv\n",
        "from torch_geometric.data import Data, DataLoader, TemporalData\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import math\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch_geometric.nn import GATv2Conv as GATConv\n"
      ],
      "metadata": {
        "id": "7QBD1VOYw96D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f3da57-cb8c-4d95-af2a-460dcc572312"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1+cu118\n",
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-3489933415>\", line 6, in <cell line: 0>\n",
            "    from torch_geometric.nn import TransformerConv, global_mean_pool, BatchNorm, GATConv\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/__init__.py\", line 21, in <module>\n",
            "    import torch_geometric.datasets\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/__init__.py\", line 18, in <module>\n",
            "    from .qm9 import QM9\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py\", line 22, in <module>\n",
            "    conversion = torch.tensor([\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/datasets/qm9.py:22: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  conversion = torch.tensor([\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"TGAT_CAN_IDS_100%_Accuracy.ipynb\n",
        "\n",
        "TGAT (Temporal Graph Attention Network) implementation for CAN bus intrusion detection\n",
        "Replicating the 99.99% accuracy achieved with GCN using temporal attention mechanisms\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class CANDataProcessor:\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_otids_data(self, file_paths):\n",
        "        all_data = []\n",
        "\n",
        "        for file_path, label in file_paths.items():\n",
        "            print(f\"Processing {file_path}...\")\n",
        "\n",
        "            try:\n",
        "                data = []\n",
        "\n",
        "                with open(file_path, 'r') as f:\n",
        "                    for line in f:\n",
        "                        line = line.strip()\n",
        "                        if not line:\n",
        "                            continue\n",
        "\n",
        "                        parts = line.split()\n",
        "                        timestamp = float(parts[1])\n",
        "                        can_id = int(parts[3], 16)\n",
        "                        dlc = int(parts[6])\n",
        "                        data_bytes = [int(byte, 16) for byte in parts[7:7 + dlc]]\n",
        "\n",
        "                        # Pad data to 8 bytes\n",
        "                        while len(data_bytes) < 8:\n",
        "                            data_bytes.append(0)\n",
        "\n",
        "                        row = [timestamp, can_id, dlc] + data_bytes\n",
        "                        data.append(row)\n",
        "\n",
        "                df = pd.DataFrame(data, columns=[\n",
        "                    'timestamp', 'can_id', 'dlc',\n",
        "                    'data0', 'data1', 'data2', 'data3',\n",
        "                    'data4', 'data5', 'data6', 'data7'\n",
        "                ])\n",
        "                df['label'] = label\n",
        "                all_data.append(df)\n",
        "\n",
        "                print(f\"✔ Loaded {len(df)} rows from {file_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error parsing {file_path}: {e}\")\n",
        "\n",
        "        if not all_data:\n",
        "            print(\"⚠ No data loaded. Returning empty DataFrame.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Create sample CAN data for demonstration if files not found\"\"\"\n",
        "        np.random.seed(42)\n",
        "        sample_data = []\n",
        "\n",
        "        # Generate sample data for each attack type\n",
        "        for label in ['normal', 'dos', 'fuzzy', 'impersonation']:\n",
        "            n_samples = 5000\n",
        "\n",
        "            # CAN message features: timestamp, can_id, dlc, data0-data7\n",
        "            if label == 'normal':\n",
        "                # Normal traffic patterns\n",
        "                timestamps = np.arange(n_samples) * 0.001  # Regular intervals\n",
        "                can_ids = np.random.choice([0x123, 0x456, 0x789], n_samples)\n",
        "                dlc = np.random.choice([8], n_samples)\n",
        "                data = np.random.randint(0, 256, (n_samples, 8))\n",
        "            elif label == 'dos':\n",
        "                # DoS attack - high frequency, same ID\n",
        "                timestamps = np.arange(n_samples) * 0.0001  # High frequency\n",
        "                can_ids = np.full(n_samples, 0x123)\n",
        "                dlc = np.random.choice([8], n_samples)\n",
        "                data = np.random.randint(0, 256, (n_samples, 8))\n",
        "            elif label == 'fuzzy':\n",
        "                # Fuzzy attack - random data with irregular timestamps\n",
        "                timestamps = np.sort(np.random.random(n_samples) * 10)\n",
        "                can_ids = np.random.randint(0, 2048, n_samples)\n",
        "                dlc = np.random.choice(range(1, 9), n_samples)\n",
        "                data = np.random.randint(0, 256, (n_samples, 8))\n",
        "            else:  # impersonation\n",
        "                # Impersonation - legitimate ID with suspicious temporal patterns\n",
        "                timestamps = np.arange(n_samples) * 0.005  # Irregular intervals\n",
        "                can_ids = np.random.choice([0x123, 0x456], n_samples)\n",
        "                dlc = np.random.choice([8], n_samples)\n",
        "                data = np.random.randint(200, 256, (n_samples, 8))\n",
        "\n",
        "            # Combine features\n",
        "            features = np.column_stack([timestamps, can_ids, dlc, data])\n",
        "            df = pd.DataFrame(features, columns=[\n",
        "                'timestamp', 'can_id', 'dlc',\n",
        "                'data0', 'data1', 'data2', 'data3',\n",
        "                'data4', 'data5', 'data6', 'data7'\n",
        "            ])\n",
        "            df['label'] = label\n",
        "            sample_data.append(df)\n",
        "\n",
        "        print(\"Created sample dataset with 20,000 CAN messages\")\n",
        "        return pd.concat(sample_data, ignore_index=True)\n",
        "\n",
        "    def preprocess_can_data(self, df):\n",
        "        \"\"\"Preprocess CAN bus data\"\"\"\n",
        "        feature_cols = [col for col in df.columns if col != 'label']\n",
        "\n",
        "        # Handle missing values\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        # Extract features\n",
        "        X = df[feature_cols].values\n",
        "        y = df['label'].values\n",
        "\n",
        "        # Normalize features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Encode labels\n",
        "        y_encoded = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        return X_scaled, y_encoded, df['timestamp'].values\n",
        "\n",
        "    def create_temporal_graph(self, X, y, timestamps, window_size=15):\n",
        "        \"\"\"Create temporal graph structure from CAN data for TGAT\"\"\"\n",
        "        graphs = []\n",
        "\n",
        "        # Sort by timestamp to ensure temporal order\n",
        "        sort_idx = np.argsort(timestamps)\n",
        "        X_sorted = X[sort_idx]\n",
        "        y_sorted = y[sort_idx]\n",
        "        timestamps_sorted = timestamps[sort_idx]\n",
        "\n",
        "        for i in range(0, len(X_sorted) - window_size + 1, window_size//2):\n",
        "            # Get window of data\n",
        "            window_data = X_sorted[i:i+window_size]\n",
        "            window_labels = y_sorted[i:i+window_size]\n",
        "            window_timestamps = timestamps_sorted[i:i+window_size]\n",
        "\n",
        "            # Create temporal adjacency matrix\n",
        "            adj_matrix = np.zeros((window_size, window_size))\n",
        "            time_matrix = np.zeros((window_size, window_size))\n",
        "\n",
        "            # Temporal connections (consecutive messages)\n",
        "            for j in range(window_size - 1):\n",
        "                adj_matrix[j, j+1] = 1.0\n",
        "                time_matrix[j, j+1] = window_timestamps[j+1] - window_timestamps[j]\n",
        "\n",
        "            # Feature similarity connections with temporal decay\n",
        "            for j in range(window_size):\n",
        "                for k in range(j+1, window_size):\n",
        "                    # Feature similarity\n",
        "                    feature_sim = np.exp(-np.linalg.norm(window_data[j] - window_data[k]))\n",
        "\n",
        "                    # Temporal decay\n",
        "                    time_diff = abs(window_timestamps[k] - window_timestamps[j])\n",
        "                    temporal_decay = np.exp(-time_diff / np.std(window_timestamps))\n",
        "\n",
        "                    # Combined similarity\n",
        "                    similarity = feature_sim * temporal_decay\n",
        "\n",
        "                    if similarity > 0.3:  # Threshold for connection\n",
        "                        adj_matrix[j, k] = similarity\n",
        "                        adj_matrix[k, j] = similarity\n",
        "                        time_matrix[j, k] = time_diff\n",
        "                        time_matrix[k, j] = time_diff\n",
        "\n",
        "            # Convert to edge index format\n",
        "            edge_indices = np.where(adj_matrix > 0)\n",
        "            edge_index = torch.tensor(np.vstack(edge_indices), dtype=torch.long)\n",
        "            edge_weight = torch.tensor(adj_matrix[edge_indices], dtype=torch.float)\n",
        "            edge_time = torch.tensor(time_matrix[edge_indices], dtype=torch.float)\n",
        "\n",
        "            # Node features with temporal information\n",
        "            node_features = torch.tensor(window_data, dtype=torch.float)\n",
        "\n",
        "            # Add temporal features\n",
        "            relative_times = torch.tensor(\n",
        "                (window_timestamps - window_timestamps[0]).reshape(-1, 1),\n",
        "                dtype=torch.float\n",
        "            )\n",
        "            node_features = torch.cat([node_features, relative_times], dim=1)\n",
        "\n",
        "            # Graph label\n",
        "            graph_label = torch.tensor(window_labels[-1], dtype=torch.long)\n",
        "\n",
        "            # Create temporal graph data object\n",
        "            graph = Data(\n",
        "                x=node_features,\n",
        "                edge_index=edge_index,\n",
        "                edge_attr=torch.cat([edge_weight.unsqueeze(1), edge_time.unsqueeze(1)], dim=1),\n",
        "                y=graph_label\n",
        "            )\n",
        "\n",
        "            graphs.append(graph)\n",
        "\n",
        "        return graphs"
      ],
      "metadata": {
        "id": "zjL29pYgxCzM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Fixed Positional encoding for graph input (2D-safe)\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
        "                             (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is [num_nodes, feature_dim]\n",
        "        pe = self.pe[:x.size(0), :]\n",
        "        return x + pe\n",
        "\n",
        "\n",
        "class TemporalGraphAttentionLayer(nn.Module):\n",
        "    \"\"\"Custom Temporal Graph Attention Layer\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, num_heads=8, dropout=0.1):\n",
        "        super(TemporalGraphAttentionLayer, self).__init__()\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.out_dim = out_dim\n",
        "        self.head_dim = out_dim // num_heads\n",
        "\n",
        "        assert out_dim % num_heads == 0\n",
        "\n",
        "        # Attention mechanisms\n",
        "        self.spatial_attention = GATConv(in_dim, out_dim, heads=num_heads,\n",
        "                                       dropout=dropout, concat=False)\n",
        "        self.temporal_attention = nn.MultiheadAttention(out_dim, num_heads,\n",
        "                                                       dropout=dropout, batch_first=True)\n",
        "\n",
        "        # Temporal encoding\n",
        "        self.temporal_encoder = nn.Linear(2, out_dim)  # edge_weight + edge_time\n",
        "\n",
        "        # Layer normalization and dropout\n",
        "        self.norm1 = nn.LayerNorm(out_dim)\n",
        "        self.norm2 = nn.LayerNorm(out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Feed forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(out_dim, out_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(out_dim * 4, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        # Spatial attention\n",
        "        spatial_out = self.spatial_attention(x, edge_index)\n",
        "        spatial_out = self.dropout(spatial_out)\n",
        "\n",
        "        # Residual connection and normalization\n",
        "        x = self.norm1(x + spatial_out)\n",
        "\n",
        "        # Temporal attention (treat nodes as sequence)\n",
        "        temporal_input = x.unsqueeze(0)  # Add batch dimension\n",
        "        temporal_out, _ = self.temporal_attention(temporal_input, temporal_input, temporal_input)\n",
        "        temporal_out = temporal_out.squeeze(0)  # Remove batch dimension\n",
        "        temporal_out = self.dropout(temporal_out)\n",
        "\n",
        "        # Residual connection and normalization\n",
        "        x = self.norm2(x + temporal_out)\n",
        "\n",
        "        # Feed forward network\n",
        "        ffn_out = self.ffn(x)\n",
        "        ffn_out = self.dropout(ffn_out)\n",
        "\n",
        "        # Final residual connection\n",
        "        return x + ffn_out\n",
        "\n",
        "class AdvancedTGAT(nn.Module):\n",
        "    \"\"\"Advanced Temporal Graph Attention Network for CAN IDS\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_classes=4, num_heads=8,\n",
        "                 num_layers=4, dropout=0.2):\n",
        "        super(AdvancedTGAT, self).__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.positional_encoding = PositionalEncoding(hidden_dim)\n",
        "\n",
        "        # Multiple TGAT layers\n",
        "        self.tgat_layers = nn.ModuleList([\n",
        "            TemporalGraphAttentionLayer(hidden_dim, hidden_dim, num_heads, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Batch normalization layers\n",
        "        self.batch_norms = nn.ModuleList([\n",
        "            BatchNorm(hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Global attention pooling\n",
        "        self.global_attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Classification head with multiple layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.BatchNorm1d(hidden_dim // 4),\n",
        "            nn.Linear(hidden_dim // 4, num_classes)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch):\n",
        "        # Input projection\n",
        "        x = self.input_projection(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        # Apply TGAT layers with residual connections\n",
        "        for i, (tgat_layer, bn) in enumerate(zip(self.tgat_layers, self.batch_norms)):\n",
        "            residual = x\n",
        "            x = tgat_layer(x, edge_index, edge_attr)\n",
        "            x = bn(x)\n",
        "\n",
        "            # Residual connection (with projection if needed)\n",
        "            if i > 0:\n",
        "                x = x + residual\n",
        "\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout(x)\n",
        "\n",
        "        # Global attention pooling\n",
        "        attention_weights = self.global_attention(x)\n",
        "        attention_weights = torch.softmax(attention_weights, dim=0)\n",
        "\n",
        "        # Apply attention weights to pooling\n",
        "        pooled_features = []\n",
        "        for batch_id in torch.unique(batch):\n",
        "            mask = (batch == batch_id)\n",
        "            batch_features = x[mask]\n",
        "            batch_weights = attention_weights[mask]\n",
        "\n",
        "            # Weighted average pooling\n",
        "            weighted_features = torch.sum(batch_features * batch_weights, dim=0)\n",
        "            pooled_features.append(weighted_features)\n",
        "\n",
        "        x_pooled = torch.stack(pooled_features)\n",
        "\n",
        "        # Classification\n",
        "        out = self.classifier(x_pooled)\n",
        "\n",
        "        return out\n",
        "\n",
        "class CANIDSTrainer:\n",
        "    def __init__(self, model, device):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=0.0005, weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='max', factor=0.7, patience=5, verbose=True\n",
        "        )\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        print(\"🔁 Entered train_epoch()\")\n",
        "        for batch in train_loader:\n",
        "\n",
        "            batch = batch.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            out = self.model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
        "            loss = self.criterion(out, batch.y)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(out.data, 1)\n",
        "            total += batch.y.size(0)\n",
        "            correct += (predicted == batch.y).sum().item()\n",
        "\n",
        "        return total_loss / len(train_loader), 100 * correct / total\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = batch.to(self.device)\n",
        "                out = self.model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
        "                _, predicted = torch.max(out.data, 1)\n",
        "\n",
        "                total += batch.y.size(0)\n",
        "                correct += (predicted == batch.y).sum().item()\n",
        "\n",
        "                all_preds.extend(predicted.cpu().tolist())\n",
        "                all_labels.extend(batch.y.cpu().tolist())\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        return accuracy, all_preds, all_labels\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=150):\n",
        "        print(f\"{'Epoch':<10}{'Train Loss':<15}{'Train Acc':<15}{'Val Acc':<15}\")\n",
        "        print(\"=\" * 55)\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = self.train_epoch(train_loader)\n",
        "            val_acc, _, _ = self.evaluate(val_loader)\n",
        "\n",
        "            self.scheduler.step(val_acc)\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(self.model.state_dict(), 'best_tgat_can_ids_model.pth')\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            print(f'Epoch {epoch:03}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "\n",
        "            # Early stopping\n",
        "            if patience_counter >= 20:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        return best_val_acc"
      ],
      "metadata": {
        "id": "hWB2Dn4hxFzV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "1eHZ6BQ5EmEI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Check if running in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Running in Google Colab\")\n",
        "        print(\"Using sample data for demonstration...\")\n",
        "        use_sample_data = True\n",
        "    except ImportError:\n",
        "        use_sample_data = False\n",
        "\n",
        "    # Define file paths - update these paths to match your dataset location\n",
        "    file_paths = {\n",
        "        '/content/drive/MyDrive/data/DoS_attack_dataset.txt': 'dos',\n",
        "        '/content/drive/MyDrive/data/Fuzzy_attack_dataset.txt': 'fuzzy',\n",
        "        '/content/drive/MyDrive/data/Impersonation_attack_dataset.txt': 'impersonation',\n",
        "        '/content/drive/MyDrive/data/Attack_free_dataset.txt': 'normal'\n",
        "    }\n",
        "\n",
        "    # Initialize data processor\n",
        "    processor = CANDataProcessor()\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading OTIDS dataset...\")\n",
        "    try:\n",
        "        df = processor.load_otids_data(file_paths)\n",
        "        if df.empty:\n",
        "            raise FileNotFoundError(\"Dataset files not found\")\n",
        "    except:\n",
        "        print(\"Dataset files not found, using sample data...\")\n",
        "        df = processor.create_sample_data()\n",
        "\n",
        "    print(f\"Total samples: {len(df)}\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
        "\n",
        "    # Preprocess data\n",
        "    print(\"\\nPreprocessing data...\")\n",
        "    X, y, timestamps = processor.preprocess_can_data(df)\n",
        "\n",
        "    # Create temporal graphs\n",
        "    print(\"Creating temporal graph structures...\")\n",
        "    graphs = processor.create_temporal_graph(X, y, timestamps, window_size=20)\n",
        "    print(f\"Created {len(graphs)} temporal graphs\")\n",
        "\n",
        "    torch.save(graphs, '/content/drive/MyDrive/otids_temporal_graphs_loaded.pt')\n",
        "    print(\"✅ Graphs saved to Drive as otids_temporal_graphs.pt\")\n",
        "\n",
        "\n",
        "    # Split data\n",
        "    train_graphs, test_graphs = train_test_split(graphs, test_size=0.2,\n",
        "                                               stratify=[g.y.item() for g in graphs],\n",
        "                                               random_state=42)\n",
        "    train_graphs, val_graphs = train_test_split(train_graphs, test_size=0.2,\n",
        "                                              stratify=[g.y.item() for g in train_graphs],\n",
        "                                              random_state=42)\n",
        "\n",
        "    print(f\"Train graphs: {len(train_graphs)}\")\n",
        "    print(f\"Validation graphs: {len(val_graphs)}\")\n",
        "    print(f\"Test graphs: {len(test_graphs)}\")\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    input_dim = graphs[0].x.shape[1]  # Features + temporal info\n",
        "    num_classes = len(np.unique(y))\n",
        "\n",
        "    model = AdvancedTGAT(input_dim=input_dim,\n",
        "                        hidden_dim=256,\n",
        "                        num_classes=num_classes,\n",
        "                        num_heads=8,\n",
        "                        num_layers=4,\n",
        "                        dropout=0.1)\n",
        "\n",
        "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "    print(f\"Input dimension: {input_dim}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = CANIDSTrainer(model, device)\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining TGAT model...\")\n",
        "    best_val_acc = trainer.train_model(train_loader, val_loader, epochs=200)\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('best_tgat_can_ids_model.pth'))\n",
        "    except:\n",
        "        print(\"Could not load saved model, using current model state\")\n",
        "\n",
        "    test_acc, test_preds, test_labels = trainer.evaluate(test_loader)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"FINAL TGAT RESULTS\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Detailed evaluation\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"CLASSIFICATION REPORT\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(classification_report(test_labels, test_preds,\n",
        "                              target_names=processor.label_encoder.classes_))\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"CONFUSION MATRIX\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(confusion_matrix(test_labels, test_preds))\n",
        "\n",
        "    return model, test_acc, graphs, processor\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the main function\n",
        "    model, accuracy, graphs, processor = main()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TGAT MODEL TRAINING COMPLETED!\")\n",
        "    print(f\"Final Test Accuracy: {accuracy:.2f}%\")\n",
        "    if accuracy >= 98.0:\n",
        "        print(\"🎉 TARGET ACCURACY OF 98% ACHIEVED WITH TGAT! 🎉\")\n",
        "    else:\n",
        "        print(\"📈 To improve TGAT accuracy further, try:\")\n",
        "        print(\"- Increase number of attention heads\")\n",
        "        print(\"- Adjust temporal window size\")\n",
        "        print(\"- Fine-tune attention mechanisms\")\n",
        "        print(\"- Increase model depth\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save TGAT model\n",
        "import shutil\n",
        "shutil.copy('best_tgat_can_ids_model.pth', '/content/drive/MyDrive/best_tgat_can_ids_model.pth')\n",
        "\n"
      ],
      "metadata": {
        "id": "XfBxQgDTxJGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "ec1c5976-0fee-40f8-fe81-7ebc3b40f5e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Running in Google Colab\n",
            "Using sample data for demonstration...\n",
            "Loading OTIDS dataset...\n",
            "Processing /content/drive/MyDrive/data/DoS_attack_dataset.txt...\n",
            "✔ Loaded 656579 rows from /content/drive/MyDrive/data/DoS_attack_dataset.txt\n",
            "Processing /content/drive/MyDrive/data/Fuzzy_attack_dataset.txt...\n",
            "✔ Loaded 591990 rows from /content/drive/MyDrive/data/Fuzzy_attack_dataset.txt\n",
            "Processing /content/drive/MyDrive/data/Impersonation_attack_dataset.txt...\n",
            "✔ Loaded 995472 rows from /content/drive/MyDrive/data/Impersonation_attack_dataset.txt\n",
            "Processing /content/drive/MyDrive/data/Attack_free_dataset.txt...\n",
            "✔ Loaded 2369398 rows from /content/drive/MyDrive/data/Attack_free_dataset.txt\n",
            "Total samples: 4613439\n",
            "Dataset shape: (4613439, 12)\n",
            "Label distribution:\n",
            "label\n",
            "normal           2369398\n",
            "impersonation     995472\n",
            "dos               656579\n",
            "fuzzy             591990\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Preprocessing data...\n",
            "Creating temporal graph structures...\n",
            "Created 461342 temporal graphs\n",
            "✅ Graphs saved to Drive as otids_temporal_graphs.pt\n",
            "Train graphs: 295258\n",
            "Validation graphs: 73815\n",
            "Test graphs: 92269\n",
            "\n",
            "Model parameters: 7462213\n",
            "Input dimension: 12\n",
            "Number of classes: 4\n",
            "\n",
            "Training TGAT model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Numpy is not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1097356148>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*60}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-1097356148>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTraining TGAT model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Load best model and evaluate on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2630848396>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2630848396>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_loader)\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m                 \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Mount Drive before anything\n",
        "\n",
        "def main():\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Define file paths - update these paths to match your dataset location\n",
        "    file_paths = {\n",
        "        '/content/drive/MyDrive/data/DoS_attack_dataset.txt': 'dos',\n",
        "        '/content/drive/MyDrive/data/Fuzzy_attack_dataset.txt': 'fuzzy',\n",
        "        '/content/drive/MyDrive/data/Impersonation_attack_dataset.txt': 'impersonation',\n",
        "        '/content/drive/MyDrive/data/Attack_free_dataset.txt': 'normal'\n",
        "    }\n",
        "\n",
        "    graph_path = '/content/drive/MyDrive/otids_temporal_graphs_loaded.pt'\n",
        "\n",
        "    # Initialize data processor\n",
        "    processor = CANDataProcessor()\n",
        "\n",
        "    # Load or create graphs\n",
        "    if os.path.exists(graph_path):\n",
        "        graphs = torch.load(graph_path)\n",
        "        print(f\"✅ Loaded {len(graphs)} temporal graphs from saved file\")\n",
        "    else:\n",
        "        print(\"Loading OTIDS dataset...\")\n",
        "        df = processor.load_otids_data(file_paths)\n",
        "        print(f\"Total samples: {len(df)}\")\n",
        "        print(f\"Dataset shape: {df.shape}\")\n",
        "        print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
        "\n",
        "        print(\"\\nPreprocessing data...\")\n",
        "        X, y, timestamps = processor.preprocess_can_data(df)\n",
        "\n",
        "        print(\"Creating temporal graph structures...\")\n",
        "        graphs = processor.create_temporal_graph(X, y, timestamps, window_size=20)\n",
        "        print(f\"Created {len(graphs)} temporal graphs\")\n",
        "        torch.save(graphs, graph_path)\n",
        "        print(\"✅ Graphs saved to Drive\")\n",
        "\n",
        "    # Split data\n",
        "    train_graphs, test_graphs = train_test_split(graphs, test_size=0.2,\n",
        "                                                 stratify=[g.y.item() for g in graphs],\n",
        "                                                 random_state=42)\n",
        "    train_graphs, val_graphs = train_test_split(train_graphs, test_size=0.2,\n",
        "                                                stratify=[g.y.item() for g in train_graphs],\n",
        "                                                random_state=42)\n",
        "\n",
        "    print(f\"Train graphs: {len(train_graphs)}\")\n",
        "    print(f\"Validation graphs: {len(val_graphs)}\")\n",
        "    print(f\"Test graphs: {len(test_graphs)}\")\n",
        "    # Temporarily reduce dataset size for faster training\n",
        "\n",
        "\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(train_graphs, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_graphs, batch_size=32, shuffle=False)\n",
        "    test_loader = DataLoader(test_graphs, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Model setup\n",
        "    input_dim = graphs[0].x.shape[1]\n",
        "    num_classes = len(set([g.y.item() for g in graphs]))\n",
        "\n",
        "    model = AdvancedTGAT(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=128,\n",
        "        num_classes=num_classes,\n",
        "        num_heads=4,\n",
        "        num_layers=2,\n",
        "        dropout=0.05\n",
        "    )\n",
        "\n",
        "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "    print(f\"Input dimension: {input_dim}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    # Training\n",
        "    trainer = CANIDSTrainer(model, device)\n",
        "    print(\"\\nTraining TGAT model...\")\n",
        "    best_val_acc = trainer.train_model(train_loader, val_loader, epochs=200)\n",
        "\n",
        "    # Save & evaluate\n",
        "    model.load_state_dict(torch.load('best_tgat_can_ids_model.pth'))\n",
        "    test_acc, test_preds, test_labels = trainer.evaluate(test_loader)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"FINAL TGAT RESULTS\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nCLASSIFICATION REPORT\")\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    print(classification_report(test_labels, test_preds))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nCONFUSION MATRIX\")\n",
        "    print(confusion_matrix(test_labels, test_preds))\n",
        "\n",
        "    return model, test_acc, graphs, processor\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model, accuracy, graphs, processor = main()\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TGAT MODEL TRAINING COMPLETED!\")\n",
        "    print(f\"Final Test Accuracy: {accuracy:.2f}%\")\n",
        "    if accuracy >= 98.0:\n",
        "        print(\"🎉 TARGET ACCURACY OF 98% ACHIEVED WITH TGAT! 🎉\")\n",
        "    else:\n",
        "        print(\"📈 To improve TGAT accuracy further, try:\")\n",
        "        print(\"- Increase number of attention heads\")\n",
        "        print(\"- Adjust temporal window size\")\n",
        "        print(\"- Fine-tune attention mechanisms\")\n",
        "        print(\"- Increase model depth\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4o6v91GTXVd",
        "outputId": "c78c7331-0401-4fc8-b82d-116d1f0df9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "✅ Loaded 461342 temporal graphs from saved file\n",
            "Train graphs: 295258\n",
            "Validation graphs: 73815\n",
            "Test graphs: 92269\n",
            "\n",
            "Model parameters: 7462213\n",
            "Input dimension: 12\n",
            "Number of classes: 4\n",
            "\n",
            "Training TGAT model...\n",
            "Epoch     Train Loss     Train Acc      Val Acc        \n",
            "=======================================================\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 000: Train Loss: 0.7510, Train Acc: 74.48%, Val Acc: 74.54%\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 001: Train Loss: 0.7498, Train Acc: 74.41%, Val Acc: 73.53%\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 002: Train Loss: 0.7379, Train Acc: 74.74%, Val Acc: 72.32%\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 003: Train Loss: 0.7312, Train Acc: 74.76%, Val Acc: 75.23%\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 004: Train Loss: 0.7325, Train Acc: 74.83%, Val Acc: 61.85%\n",
            "🔁 Entered train_epoch()\n",
            "Epoch 005: Train Loss: 0.7332, Train Acc: 74.75%, Val Acc: 73.92%\n",
            "🔁 Entered train_epoch()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('best_tgat_can_ids_model.pth', '/content/drive/MyDrive/best_tgat_can_ids_model.pth')\n",
        "print(\"✅ TGAT model saved to Drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uumvbAlVel1F",
        "outputId": "ea84f1c6-c763-40ff-9c01-333a58d8ed92"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ TGAT model saved to Drive\n"
          ]
        }
      ]
    }
  ]
}