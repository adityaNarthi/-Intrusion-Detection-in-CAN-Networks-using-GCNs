{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCiv2jZ2MnVD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty list to store the entries\n",
        "entries = []\n",
        "\n",
        "# List of file paths for each data type\n",
        "files = [\n",
        "    ('/content/drive/MyDrive/impersonating.txt', 1),  # Label 1 for Impersonating\n",
        "    ('/content/drive/MyDrive/dos.txt', 2),          # Label 2 for DoS\n",
        "    ('/content/drive/MyDrive/fuzzy.txt', 3),        # Label 3 for Fuzzy\n",
        "    ('/content/drive/MyDrive/attack_free.txt', 0)   # Label 0 for Attack Free\n",
        "]\n",
        "\n",
        "# Loop through each file and parse it\n",
        "for file, label in files:\n",
        "    with open(file, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < 10:\n",
        "                continue\n",
        "            ts      = float(parts[1])\n",
        "            node_id = int(parts[3], 16)\n",
        "            dlc     = int(parts[parts.index('DLC:') + 1])\n",
        "            entries.append((ts, node_id, dlc, label))  # Add label to the entry\n",
        "\n",
        "# Create DataFrame from the entries\n",
        "df = pd.DataFrame(entries, columns=['ts', 'src', 'dlc', 'label'])\n",
        "df.sort_values('ts', inplace=True)\n",
        "df['dt'] = df['ts'].diff().fillna(0.0)\n",
        "\n",
        "# Feature Engineering (dt + one-hot DLC)\n",
        "one_hot = pd.get_dummies(df['dlc'], prefix='dlc').astype('float32')\n",
        "features = pd.concat([df[['dt']], one_hot], axis=1).values.astype('float32')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Build the four lists (plus placeholder labels if needed)\n",
        "src     = df['src'].tolist()                                     # source node IDs\n",
        "dst     = src                                                   # destination (selfâ€‘edges)\n",
        "ts_list = df['ts'].tolist()                                     # timestamps\n",
        "x_feats = [torch.tensor(f, dtype=torch.float32) for f in features]  # feature vectors\n",
        "y       = df['label'].tolist()                                  # labels (0 for Attack Free, 1 for Impersonating, etc.)\n",
        "\n",
        "# 4. Print the first few entries of each list\n",
        "print(\"First 5 source nodes (src):\", src[:5])\n",
        "print(\"First 5 destination nodes (dst):\", dst[:5])\n",
        "print(\"First 5 timestamps (ts_list):\", ts_list[:5])\n",
        "print(\"First 5 feature vectors (x_feats):\")\n",
        "for vec in x_feats[:5]:\n",
        "    print(\" \", vec)\n"
      ],
      "metadata": {
        "id": "bTg9In9fNC_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric_temporal.signal import DynamicGraphTemporalSignal\n",
        "import torch\n",
        "\n",
        "# 5. Wrap the data into DynamicGraphTemporalSignal format\n",
        "edge_indices = [torch.tensor([[s], [d]], dtype=torch.long) for s, d in zip(src, dst)]\n",
        "edge_attrs   = x_feats\n",
        "timestamps   = [torch.tensor([t], dtype=torch.float32) for t in ts_list]\n",
        "labels       = [torch.tensor([yy], dtype=torch.long) for yy in y]\n",
        "\n",
        "# Create the dataset\n",
        "dataset = DynamicGraphTemporalSignal(\n",
        "    edge_indices=edge_indices,\n",
        "    edge_attrs=edge_attrs,\n",
        "    timestamps=timestamps,\n",
        "    y=labels\n",
        ")\n",
        "\n",
        "# Print the first few entries of the dataset to verify\n",
        "print(\"Dataset: \", dataset)\n"
      ],
      "metadata": {
        "id": "LKSqd134NGJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric_temporal.nn import TGAT\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize your TGAT model (replace with your specific architecture)\n",
        "model = TGAT(in_channels=features.shape[1], out_channels=64)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in dataset:\n",
        "        # Forward pass\n",
        "        out = model(data.x, data.edge_index, data.edge_attr, data.timestamps)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(out, data.y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataset)}')\n"
      ],
      "metadata": {
        "id": "FAcxuF6ANIZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}